{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the gpt2 tokenizer\n",
    "TOKENIZER: PreTrainedTokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257 \n",
      "len(tokens_with_quotes) = 131\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(TOKENIZER)} \")\n",
    "tokens_with_quotes: list[str] = [\n",
    "\tx \n",
    "    for x in TOKENIZER.get_vocab().keys() \n",
    "    if ('\"' in x)\n",
    "]\n",
    "print(f\"{len(tokens_with_quotes) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"', 'Ġ\"-', 'Ġ\"[', '\"]=>', 'Ġ\"_', '=\"#', '\">', '\"\"\"', '=\"', '?\\'\"', 'Ġ\"#', '\\',\"', '\":\"\",\"', '\",', '/\"', '.\"\"', 'Ġ.\"', 'Ġ,\"', '\"/>', '\":[\"', '\").', '!\"', 'Ġ\"\"\"', '\"!', 'Ġ\"(', 'âĢĶ\"', '=\"/', '\"âĢ¦', ';\"', 'Ġ\"$:/', 'âĢ¦.\"', '\":', '!!\"', '\"><', '\".', '\"-', '.\",', ',\"', 'Ġ[\"', '%\"', 'Ġ(\"', '=\"\"', '\"âĢĶ', '.\"', '\"],\"', '\\\\\",', '\"},', '].\"', '\\\\\">', '?!\"', '\\'\"', '\"}],\"', ').\"', 'Ġ\"\\'', '\"],', '!\",', '\":-', '?\".', 'Ġ...\"', '=\\\\\"', 'Ġ\"...', ':\"', '!\".', '(\"', '},{\"', '\"))', '!\\'\"', '\"),', ',\\'\"', 'Ġ\".', '\")', '\":\"/', '...\"', '.\"[', '.\\'\"', 'Ġ\"{', '\"...', '\"},\"', 'Ġ\"\\\\', '\"></', ']\"', '.\",\"', '\");', '.,\"', '\".[', '},\"', 'Ġ\\\\\"', '?\"', 'Ġ\"<', '?\",', '\\\\\":', '>\"', '\\\\\"', 'Ġ\"+', '\":[', '\"\\'', '\"},{\"', '\":{\"', '!?\"', '),\"', '\",\"', '[\"', 'Ġ{\"', '\"[', '\"]', 'âĢ¦\"', 'Ġ\"', '\":\"', 'Ġ\"%', '\"?', '\";', '.\")', '\":\"\"},{\"', 'Ġ\"\"', '}\"', 'ĠâĢ¦\"', '\"\"', 'Ġ\"$', '\":[{\"', 'Ġ\",', '\"', '-\"', '\"(', ')\"', 'Ġ\"/', '],\"', '\"}', '\\'.\"', ')\",', 'Ġ\"âĢ¦', 'Ġ\"@']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_with_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a subset of the tinystories dataset\n",
    "with open(\"../data/tiny_stories/tinystories_10k.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    TEXT_DATA: list[str] = f.read().split(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into tokens but don't convert to ids\n",
    "text_data_tokenized = [\n",
    "\tTOKENIZER(x).tokens()\n",
    "\tfor x in TEXT_DATA\n",
    "]\n",
    "\n",
    "from itertools import chain\n",
    "text_data_tokenized_joined = list(chain(*text_data_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text_data_tokenized) = 1728\n",
      "len(text_data_tokenized_joined) = 349321\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(text_data_tokenized) = }\")\n",
    "print(f\"{len(text_data_tokenized_joined) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2983\t`Ġ\"`\n",
      "1262\t`!\"`\n",
      "1340\t`.\"`\n",
      "702\t`?\"`\n",
      "807\t`\"`\n",
      "364\t`,\"`\n",
      "41\t`\".`\n",
      "3\t`?!\"`\n",
      "3\t`?\".`\n",
      "2\t`'.\"`\n",
      "2\t`?\",`\n",
      "1\t`...\"`\n",
      "3\t`\",`\n",
      "5\t`!\".`\n",
      "1\t`Ġ\"'`\n",
      "1\t`',\"`\n"
     ]
    }
   ],
   "source": [
    "# set of tokens with quotes which appear in the dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "quote_tokens_in_data = Counter([\n",
    "\tx\n",
    "\tfor x in text_data_tokenized_joined\n",
    "\tif ('\"' in x)\n",
    "])\n",
    "\n",
    "\n",
    "# print(f\"{quote_tokens_in_data = }\")\n",
    "for x in quote_tokens_in_data:\n",
    "\tprint(f\"{quote_tokens_in_data[x]}\\t`{x}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "ĊĊ Ċ S ara Ġand ĠBen Ġwanted Ġto Ġdecor ate Ġa Ġbowl Ġfor Ġtheir Ġmom . ĠThey Ġfound Ġa Ġbig Ġbowl Ġin Ġthe Ġkitchen Ġand Ġsome Ġpaint Ġand Ġbrushes . ĠThey Ġtook Ġthe Ġbowl Ġand Ġthe Ġpaint Ġto Ġthe Ġbackyard Ġand Ġput Ġthem Ġon Ġa Ġtable . Ċ \" \u001b[44mLet\u001b[0m \u001b[44m's\u001b[0m \u001b[44mĠmake\u001b[0m \u001b[44mĠthe\u001b[0m \u001b[44mĠbowl\u001b[0m \u001b[44mĠpretty\u001b[0m \u001b[44mĠwith\u001b[0m \u001b[44mĠcolors\u001b[0m ,\" ĠSara Ġsaid . Ċ \" \u001b[44mOK\u001b[0m \u001b[44m,\u001b[0m \u001b[44mĠI\u001b[0m \u001b[44mĠwill\u001b[0m \u001b[44mĠpaint\u001b[0m \u001b[44mĠa\u001b[0m \u001b[44mĠflower\u001b[0m ,\" ĠBen Ġsaid . Ċ They Ġstarted Ġto Ġpaint Ġthe Ġbowl Ġwith Ġdifferent Ġcolors . ĠSara Ġpainted Ġa Ġred Ġheart Ġand ĠBen Ġpainted Ġa Ġyellow Ġflower . ĠThey Ġwere Ġhaving Ġfun . Ċ But Ġthen , Ġit Ġstarted Ġto Ġrain . ĠThe Ġrain Ġwas Ġwet Ġand Ġcold . ĠIt Ġmade Ġthe Ġpaint Ġrun Ġand Ġdrip . ĠThe Ġbowl Ġlooked Ġmessy Ġand Ġugly . Ċ \" \u001b[44mOh\u001b[0m \u001b[44mĠno\u001b[0m \u001b[44m,\u001b[0m \u001b[44mĠthe\u001b[0m \u001b[44mĠrain\u001b[0m \u001b[44mĠruined\u001b[0m \u001b[44mĠour\u001b[0m \u001b[44mĠbowl\u001b[0m !\" ĠSara Ġcried . Ċ \" \u001b[44mMom\u001b[0m \u001b[44mĠwill\u001b[0m \u001b[44mĠnot\u001b[0m \u001b[44mĠlike\u001b[0m \u001b[44mĠit\u001b[0m ,\" ĠBen Ġsaid . Ċ They Ġran Ġinside Ġthe Ġhouse Ġwith Ġthe Ġbowl . ĠThey Ġwere Ġsad Ġand Ġwet . Ċ They Ġshowed Ġthe Ġbowl Ġto Ġtheir Ġmom . ĠThey Ġsaid Ġthey Ġwere Ġsorry . Ċ But Ġmom Ġsmiled Ġand Ġhugged Ġthem . ĠShe Ġsaid Ġshe Ġloved Ġthe Ġbowl Ġand Ġthem . Ċ \" \u001b[44mIt\u001b[0m \u001b[44m's\u001b[0m \u001b[44mĠa\u001b[0m \u001b[44mĠbeautiful\u001b[0m \u001b[44mĠbowl\u001b[0m ,\" Ġshe Ġsaid . Ġ\" \u001b[44mYou\u001b[0m \u001b[44mĠmade\u001b[0m \u001b[44mĠit\u001b[0m \u001b[44mĠwith\u001b[0m \u001b[44mĠlove\u001b[0m \u001b[44mĠand\u001b[0m \u001b[44mĠcreativity\u001b[0m \u001b[44m.\u001b[0m \u001b[44mĠThe\u001b[0m \u001b[44mĠrain\u001b[0m \u001b[44mĠmade\u001b[0m \u001b[44mĠit\u001b[0m \u001b[44mĠspecial\u001b[0m \u001b[44m.\u001b[0m \u001b[44mĠIt\u001b[0m \u001b[44m's\u001b[0m \u001b[44mĠlike\u001b[0m \u001b[44mĠa\u001b[0m \u001b[44mĠrainbow\u001b[0m \u001b[44mĠbowl\u001b[0m .\" Ċ S ara Ġand ĠBen Ġfelt Ġhappy . ĠThey Ġgave Ġmom Ġa Ġkiss Ġand Ġthanked Ġher . ĠThey Ġlearned Ġthat Ġsometimes , Ġthings Ġcan Ġbe Ġgood Ġeven Ġwhen Ġthey Ġseem Ġbad . Ċ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def process_text(text):\n",
    "    # Load the GPT-2 tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Convert tokens to IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Compute binary feature for each token (1 if inside a quote, 0 otherwise)\n",
    "    quote_feature = []\n",
    "    inside_quote: bool = False\n",
    "    is_quote: bool = False\n",
    "    for token in tokens:\n",
    "        if \"\\\"\" in token:\n",
    "            is_quote = True\n",
    "            inside_quote = not inside_quote\n",
    "        else:\n",
    "            is_quote = False\n",
    "        quote_feature.append(\n",
    "            (1 if not is_quote else 0) # quotes are not inside the quote\n",
    "            if inside_quote \n",
    "            else 0\n",
    "        ) \n",
    "\n",
    "    # Convert token IDs and quote feature to tensors\n",
    "    token_ids_tensor = torch.tensor(token_ids)\n",
    "    quote_feature_tensor = torch.tensor(quote_feature)\n",
    "\n",
    "    return tokens, token_ids_tensor, quote_feature_tensor\n",
    "\n",
    "\n",
    "def display_results(text):\n",
    "    tokens, _, quote_feature_tensor = process_text(text)\n",
    "\n",
    "    print(\"Input text:\")\n",
    "    for token, feature in zip(tokens, quote_feature_tensor):\n",
    "        if feature == 1:\n",
    "            # print(f\"\\033[43m{token}\\033[0m\", end=\" \")\n",
    "            # dark blue background\n",
    "            print(f\"\\033[44m{token}\\033[0m\", end=\" \")\n",
    "        else:\n",
    "            print(token, end=\" \")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "display_results(TEXT_DATA[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
