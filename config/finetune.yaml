# Output configuration
source_dir: 'checkpoints/tiny_stories'
dest_dir: 'checkpoints/finetune'
max_iters: 500
eval_interval: 400
eval_iters: 20
log_interval: 400

# Checkpointing
always_save_checkpoint: false
never_save_checkpoint: false

# Wandb logging
wandb_log: true
wandb_project: 'tiny-stories-train'
wandb_run_name: 'mini-gpt-test-finetune'

# Dataset config
dataset: 'tiny_stories'
compile: true
gradient_accumulation_steps: 2
use_gradient_checkpointing: false
auto_tune_batch_size: false
batch_size: 16
block_size: 512
hf_config_block_size: 2048
dtype: 'bfloat16'

# Model architecture
n_layer: 4
n_head: 16
n_embd: 768
attn_dropout: 0.0
resid_dropout: 0.0
bias: false

# Optimizer
learning_rate: 3e-4
decay_lr: true
lr_decay_iters: 50
min_lr: 3e-5
beta2: 0.99
warmup_iters: 200

# Training components
train_probes: false
train_model: true

# Probe config
probe_type: 'cosine'
probe_learning_rate: 1e-3
lambda_adversarial: 1e-1
phi_probe_steps_per_model_update: 1
train_adversarially: false

# Initialization
init_from: 'huggingface'
huggingface_model_id: 'roneneldan/TinyStories-33M'

# Debugging
debug_data: false
show_final_eval_on_stop: false