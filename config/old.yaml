# Output configuration
out_dir: 'checkpoints/backup-checkpoint-24-02'
special_out_dir: 'checkpoints/tiny_stories_adv'
eval_interval: 2000
log_interval: 1
eval_iters: 200
eval_only: False

# Checkpointing
always_save_checkpoint: True
never_save_checkpoint: False
init_from: 'scratch'

# Wandb logging
wandb_log: True
wandb_project: 'owt'
wandb_run_name: 'gpt2'

# Dataset config
dataset: 'tiny_stories'
gradient_accumulation_steps: 40
batch_size: 12
block_size: 1024

# Model architecture
n_layer: 12
n_head: 12
n_embd: 768
dropout: 0.0
bias: False

# Optimizer
learning_rate: 6e-4
max_iters: 600000
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
grad_clip: 1.0
decay_lr: True
warmup_iters: 2000
lr_decay_iters: 600000
min_lr: 6e-5

# Probe settings
probe_type: 'linear'
probe_learning_rate: 1e-3
lambda_adversarial: 1e-3
train_adversarially: True

# Training components
train_model: True
train_probes: True

# System settings
backend: 'nccl'
compile: True  # if torch.cuda.is_available() else False
dtype: 'bfloat16'  # if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'