# Output configuration
source_dir: 'checkpoints/tiny_stories'
dest_dir: 'checkpoints/train'
max_iters: 6000
eval_interval: 500
eval_iters: 20
log_interval: 500

# Checkpointing
always_save_checkpoint: false
never_save_checkpoint: false

# Wandb logging
wandb_log: true
wandb_project: 'tiny-stories-train'
wandb_run_name: 'mini-gpt-test-train'

# Dataset config
dataset: 'tiny_stories'
compile: true
gradient_accumulation_steps: 2
use_gradient_checkpointing: false
auto_tune_batch_size: false
batch_size: 64
block_size: 256
dtype: 'bfloat16'

# Model architecture
n_layer: 4
n_head: 16
n_embd: 768
attn_dropout: 0.2
resid_dropout: 0.2
bias: false

# Optimizer
learning_rate: 3e-4
decay_lr: true
lr_decay_iters: 6000
min_lr: 3e-5
beta2: 0.999
warmup_iters: 500

# Training components
train_probes: false
train_model: true

# Probe config
probe_type: 'cosine'
probe_learning_rate: 1e-3
lambda_adversarial: 1e-1
phi_probe_steps_per_model_update: 1
train_adversarially: false

# Initialization
init_from: 'scratch'

# Debugging
debug_data: false
show_final_eval_on_stop: false