# Output configuration
source_dir: 'checkpoints/tiny_stories'
dest_dir: 'checkpoints/tiny_stories_adv'
max_iters: 4000
eval_interval: 400
eval_iters: 20
log_interval: 400

# Checkpointing
always_save_checkpoint: false
never_save_checkpoint: true

# Wandb logging
wandb_log: true
wandb_project: 'tiny-stories-train'
wandb_run_name: 'mini-gpt-adversarial'

# Dataset config
dataset: 'tiny_stories'
compile: true
gradient_accumulation_steps: 2
use_gradient_checkpointing: false
auto_tune_batch_size: false
batch_size: 16
block_size: 512
hf_config_block_size: 2048
dtype: 'bfloat16'

# Model architecture
n_layer: 4
n_head: 16
n_embd: 768
attn_dropout: 0.0
resid_dropout: 0.0
bias: false

# Optimizer
learning_rate: 1e-4
decay_lr: true
lr_decay_iters: 2000
min_lr: 1e-5
beta2: 0.999
warmup_iters: 200 # skip warmup when not training the model

# Training components
train_probes: true
train_model: true

# Probe config
probe_type: 'linear'
probe_learning_rate: 1e-3
lambda_adversarial: 0.1
phi_probe_steps_per_model_update: 5
train_adversarially: true

# Initialization
init_from: 'huggingface'
huggingface_model_id: 'roneneldan/TinyStories-33M'

# Debugging
debug_data: false
show_final_eval_on_stop: false